# Ensemble-Techniques
🧠 Ensemble Techniques in Machine Learning This repository explores and implements ensemble learning techniques, a powerful strategy that combines multiple machine learning models to produce better predictive performance than individual models.
Here’s a clean and professional **GitHub project description** for **"Ensemble Techniques"**, suitable for your repository's README section:

---

## 🧠 Ensemble Techniques in Machine Learning

This repository explores and implements **ensemble learning techniques**, a powerful strategy that combines multiple machine learning models to produce better predictive performance than individual models.

### 📌 Techniques Covered:

* **Bagging** (Bootstrap Aggregation)

  * Random Forest
  * Extra Trees Classifier

* **Boosting**

  * AdaBoost
  * Gradient Boosting
  * XGBoost
  * LightGBM
  * CatBoost

* **Stacking**

  * Model stacking using meta-learners

* **Voting**

  * Hard and soft voting classifiers

### 📊 Highlights:

* Comparison of model performance on classification and regression tasks
* Visualization of bias-variance trade-off
* Hyperparameter tuning and model evaluation
* Custom ensemble pipelines using `scikit-learn`

### 🧪 Datasets Used:

* Iris
* Titanic
* Breast Cancer
* Boston Housing / California Housing (for regression)

### 🛠️ Tech Stack:

* Python
* scikit-learn
* XGBoost, LightGBM, CatBoost
* Pandas, NumPy, Matplotlib, Seaborn

---


